{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import meerkat as mk\n",
    "from domino import DominoSlicer\n",
    "from classifer import *\n",
    "from bootstrap_utils import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from huggingface_hub import login\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192c971",
   "metadata": {},
   "source": [
    "This file contains example code from the LLM experiments performed in this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a328ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error slice from previous iteration\n",
    "results = pd.read_csv(\"/vol/bitbucket/yl28218/thesis/auditing_pipeline/results/results_corr.csv\")\n",
    "# separe the error slice  based on the id\n",
    "#types = ['slice_image_only', 'slice_image_text','slice_image_text_meta','slice_report_text','slice_metadata','slice_report_metadata','slice_image_metadata']\n",
    "slice_1 = results[results['id'] == 1]\n",
    "slice_2 = results[results['id'] == 2]\n",
    "slice_3 = results[results['id'] == 3]\n",
    "slice_4 = results[results['id'] == 4]\n",
    "slice_5 = results[results['id'] == 5]\n",
    "slice_6 = results[results['id'] == 6]\n",
    "slice_7 = results[results['id'] == 7]\n",
    "\n",
    "k =5\n",
    "max_features = 1000\n",
    "tokens_slice_1 = analyze_error_slice_tokens(slice_1, results, k, max_features)\n",
    "tokens_slice_2 = analyze_error_slice_tokens(slice_2, results, k, max_features)\n",
    "tokens_slice_3 = analyze_error_slice_tokens(slice_3, results, k, max_features)\n",
    "tokens_slice_4 = analyze_error_slice_tokens(slice_4, results, k, max_features)\n",
    "tokens_slice_5 = analyze_error_slice_tokens(slice_5, results, k, max_features)\n",
    "tokens_slice_6 = analyze_error_slice_tokens(slice_6, results, k, max_features)\n",
    "tokens_slice_7 = analyze_error_slice_tokens(slice_7, results, k, max_features)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer_description = \"Pneumothorax\"\n",
    "data = \"Chest X-ray\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34e3a4d",
   "metadata": {},
   "source": [
    "### Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07042078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model_id=\"google/gemma-2-2b-it\", hf_token=None,\n",
    "               hf_home=\"/vol/bitbucket/yl28218/hf_home\",\n",
    "               hf_cache=\"/vol/bitbucket/yl28218/hf_cache\"):\n",
    "\n",
    "\n",
    "    os.environ[\"HF_HOME\"] = hf_home\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = hf_cache\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        token=hf_token\n",
    "        device_map=\"auto\",     \n",
    "        torch_dtype=\"auto\"     \n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def generate_text(prompt, tokenizer, model,\n",
    "                  max_new_tokens=200,\n",
    "                  temperature=0.7,\n",
    "                  top_p=0.9):\n",
    " \n",
    "   \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "  \n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    if tokenizer.eos_token_id is not None:\n",
    "        gen_kwargs[\"eos_token_id\"] = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "   \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb95914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please replace with your own Hugging Face token\n",
    "hf_token = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxx\" \n",
    "tokenizer, model = init_model(hf_token=hf_token)\n",
    "# example usage\n",
    "print(generate_text(f\"An  slice, {slice_1[['report_text', 'metadata_description']]}, was found using a slice discovery method.Please identify the 3 most common features  In the format of H1, H2, H3.Please be specific and concise in your response.\", tokenizer, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37025409",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = f\"\"\"We are trying to audit a {classifer_description} image classifier.\n",
    "An error slice, {slice_1[['report_text', 'metadata_description']]}, was found using a slice discovery method.\n",
    "\n",
    "Please identify the 3 most important features in this slice that may be causing the classifier to fail. In the format of H1, H2, H3. Please be specific and concise in your response.\"\"\"\n",
    "print(generate_text(input_text, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_2 = f\"\"\"We are trying to audit a {classifer_description} image classifier.\n",
    "An error slice, {slice_1[['report_text', 'metadata_description']]}, was found using a slice discovery method.\n",
    "The tokens analysis also done to compare the token frequency in the error slice and the normal slice. They are ranked by the difference in frequency between the error slice and the normal slice.\n",
    "The results are {tokens_slice_1}.\n",
    "Please use the tokens found and the report text and metadata description to identify the 3 most important features in this slice that may be causing the classifier to fail.\n",
    "Please identify the 3 most important features in this slice that may be causing the classifier to fail. In the format of H1, H2, H3.Please be specific and concise in your response.\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6a063e",
   "metadata": {},
   "source": [
    "### Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea712714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "# please replace the api_key with your own key\n",
    "client = OpenAI(api_key=\"xxxxxxxxxxxxxxxxxxxxxxx\", base_url=\"https://api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c799d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "\n",
    "An  slice, {slice_1[['report_text', 'metadata_description']]}, was found using a slice discovery method.\n",
    "\n",
    "Please identify the 3 most common features  In the format of H1, H2, H3.Please be specific and concise in your response.\n",
    "\"\"\"}\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(\"Response from DeepSeek:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440fab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "We are trying to audit a {classifer_description} image classifier.\n",
    "An error slice, {slice_1[['report_text', 'metadata_description']]}, was found using a slice discovery method.\n",
    "The tokens analysis also done to compare the token frequency in the error slice and the normal slice. They are ranked by the difference in frequency between the error slice and the normal slice.\n",
    "The results are {tokens_slice_1.head(10)}.\n",
    "Please use the tokens found and the report text and metadata description to identify the 3 most important features in this slice that may be causing the classifier to fail.\n",
    "\n",
    "Please identify the 3 most important features in this slice that may be causing the classifier to fail. In the format of H1, H2, H3.Please be specific and concise in your response.\n",
    "\"\"\"}\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(\"Response from DeepSeek:\", response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
